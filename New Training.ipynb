{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8e3cdb",
   "metadata": {},
   "source": [
    "# new training notebook\n",
    "this notebook will be used to train the model on the new dataset you'll provide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719be0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup: imports and env\n",
    "import sys, platform\n",
    "import numpy as np, pandas as pd\n",
    "print('python:', platform.python_version())\n",
    "print('numpy:', np.__version__)\n",
    "print('pandas:', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a07a7d",
   "metadata": {},
   "source": [
    "## data loading\n",
    "we'll add the exact loading logic once you share the new dataset path and schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa2142c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder: load new dataset\n",
    "# df = pd.read_csv('path/to/new_dataset.csv')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71731669",
   "metadata": {},
   "source": [
    "## preprocessing & split\n",
    "we'll define preprocessing and train/test split after reviewing the new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6bde48",
   "metadata": {},
   "source": [
    "## model training\n",
    "we can start with a random forest baseline and tune later."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
